{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python beginners course - Level 2 - Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we have seen how to handle 1- and 2-dimensional numerical data, and how to manipulate the data, perform calculations on the data and calculate statistics of the data. Even though 2-dimensional Numpy are technically the same as the data often used in Excel sheets, they do feel a lot different. Some significant differences between the two are for example that we do not have column names, or that we can only use Numpy arrays to represent numerical data. In order to solve these issues and represent the data in a way that is identical to Excel, the ```Pandas``` package was introduced.\n",
    "\n",
    "The ```Pandas``` package is the most important tool for Data Scientists and Analysts working in Python today. The powerful machine learning and glamorous visualization tools may get all the attention, but pandas is the backbone of most projects in which data is used. \n",
    "\n",
    ">\\[*pandas*\\] is derived from the term \"**pan**el **da**ta\", an econometrics term for data sets that include observations over multiple time periods for the same individuals. — [Wikipedia](https://en.wikipedia.org/wiki/Pandas_%28software%29)\n",
    "\n",
    "If you're thinking about data science as a career, then it is imperative that one of the first things you do is learn pandas. In this course, we will go over the essential functionality that the package has to offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What's Pandas for?\n",
    "\n",
    "Pandas has so many uses that it might make sense to list the things it can't do instead of what it can do. It is essentially your data’s home. Through pandas, you get acquainted with your data by cleaning, transforming, and analyzing it.\n",
    "\n",
    "For example, if you want to explore a dataset stored in a CSV / Excel file on your computer then pandas will extract the data from that CSV into what it calls a **DataFrame** and then let you do things like:\n",
    "\n",
    "- Calculate statistics and answer questions about the data, like:\n",
    "\n",
    "\n",
    "    - What's the average, median, max, or min of each column? \n",
    "    - Does column A correlate with column B?\n",
    "    - What does the distribution of data in column C look like?\n",
    "    - Are there any missing values in the data?\n",
    "    - Do the columns contain integers? Decimal numbers (floating points)? Text (strings)?\n",
    "\n",
    "\n",
    "- Clean the data by doing things like removing missing values and filtering rows or columns by some criteria.\n",
    "\n",
    "\n",
    "- Visualize the data with help from Matplotlib (see notebook 3). Plot bars charts, line charts, histograms, scatter plots, and more. \n",
    "\n",
    "\n",
    "- Store the cleaned, transformed data back into a CSV or Excel file for later use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does pandas fit into the data science toolkit?\n",
    "\n",
    "Not only is the pandas library a central component of the data science toolkit but it is used in conjunction with other libraries in that collection. \n",
    "\n",
    "```Pandas``` is built on top of the ```NumPy``` package (see notebook level 1), meaning a lot of the structure of ```NumPy``` is used or replicated in ```Pandas```. Data in ```Pandas``` is often used to create visualisations in ```Matplotlib``` (see notebook level 3) and run machine learning algorithms in ```Scikit-learn``` (see notebook level 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT: BEFORE STARTING, RUN THE CELL BELOW\n",
    "Just like the previous notebook, we should always start by loading (importing) the packages. Run the cell below before you do anything else!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pandas packages as 'pd'\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core components of pandas: Series and DataFrames\n",
    "\n",
    "The primary two components of pandas are the `Series` and `DataFrame`. \n",
    "\n",
    "A `Series` is essentially a column, and a `DataFrame` is a table made up of a collection of Series. \n",
    "\n",
    "<img src=\"../assets/series-and-dataframe.png\" width=600px />\n",
    "\n",
    "DataFrames and Series are quite similar in that many operations that you can do with one you can do with the other, such as filling in missing values and calculating the mean.\n",
    "\n",
    "You'll see how these components work when we start working with data below. \n",
    "\n",
    "## 2. Creating DataFrames from scratch\n",
    "Creating DataFrames directly in Python is useful for practice and for testing new methods and functions.\n",
    "\n",
    "There are *many* ways to create a DataFrame from scratch, but a great option is to just use a simple `dict` (dictionary). A dictionary is a mapping of unique keys (names) to values.\n",
    "\n",
    "Let's say we have a fruit stand that sells apples and oranges. We want to have a column for each fruit and a row for each customer purchase. To organize this as a dictionary for pandas we could do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'names': ['June', 'Robert', 'Lily', 'David'],\n",
    "    'apples': [3, 2, 0, 1], \n",
    "    'oranges': [0, 3, 7, 2]\n",
    "}\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, *apples* and *oranges* are called the **keys** of the dictionary, while the lists on the right-hand side are called **values** of the dictionary.\n",
    "\n",
    "Now, we convert the dictionary into a DataFrame called *purchases*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "purchases = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How did that work?**\n",
    "\n",
    "Each *(key, value)* item in the dictionary corresponds to a *column* in the resulting DataFrame.\n",
    "\n",
    "In this case, the **index** of this DataFrame was assigned automatically by pandas as the numbers 0 to 3. However, we could also create our own when we initialize the DataFrame. \n",
    "\n",
    "Let's have customer names as our index: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases = pd.DataFrame(data)\n",
    "purchases = purchases.set_index('names')\n",
    "\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you'll find that most CSVs won't have a column suitable to be used as an index. That is no problem, as in that case numbers will assigned by default (starting at 0) are just fine to work with.\n",
    "\n",
    "Setting the *names* column as the index of the DataFrame allows us to **loc**ate a customer's order by using their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the purchases of customer 'Robert' by using the .loc function\n",
    "purchases.loc['Robert']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's more on locating and extracting data from the DataFrame later, but now you should be able to create a DataFrame with any random data to practice on.\n",
    "\n",
    "Let's move on to some quick methods for creating DataFrames from various other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Above we have seen how to manually create a DataFrame. Now lets try ourselves! Complete the steps below by filling in the ___.\n",
    "\n",
    "#### Step 1\n",
    "Create a DataFrame of the following table:\n",
    "\n",
    "| Account holder | Account number | Balance |\n",
    "| --- | --- | --- |\n",
    "| Kenneth Effe | 3421 1242 3232 5324 | 232.2 |\n",
    "| James Chang | 7583 0274 7537 9613 | 405.37 |\n",
    "| Ralph Mallets | 8534 7319 4697 1254 | 342.65 |\n",
    "\n",
    "Replace the ```___``` in the following cell to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1\n",
    "data = {\n",
    "    ___\n",
    "}\n",
    "\n",
    "bankaccounts = pd.DataFrame(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "Set the first column (Account holder) as the index column.\n",
    "Replace the ```___``` in the following cell to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2\n",
    "bankaccounts = bankaccounts.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "Locate Ralph's account number and balance.\n",
    "\n",
    "Replace the ```___``` in the following cell to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3\n",
    "located_account = bankaccounts.loc___\n",
    "\n",
    "print(located_account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 (challenge / optional)\n",
    "Calculate the sum and the average of all balances. Hint: use the .sum() and .mean() functions.\n",
    "\n",
    "Replace the ```___``` in the following cell to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4\n",
    "sum = bankaccounts.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to read in data\n",
    "In the examples above, we have created dataframes manually by typing out our data. This was usefull for illustrative purporses, but when we are handling large amounts of data we want this to be automated: we want to read in data from spreadsheets/CSV files.\n",
    "\n",
    "In the following examples we'll keep using our apples and oranges data, but this time it's coming from a CSV file. With CSV files (Excel-like files) all you need is a single line to load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases = pd.read_csv('../assets/purchases.csv')\n",
    "\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we want the names to be used as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../assets/purchases.csv')\n",
    "df.set_index('names')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring your data in pandas\n",
    "Now that we have a basic idea of how to load data into DataFrames, let's move on to importing some real-world data and detailing a few of the operations you'll be using a lot.\n",
    "\n",
    "DataFrames possess hundreds of methods and other operations that are crucial to any analysis. As a beginner, you should know the operations that perform simple transformations of your data and those that provide fundamental statistical functionality.\n",
    "\n",
    "In this training, we use the Boston Housing Dataset. This is a dataset often used for practice by data scientist. The aim of the dataset is to predict house prices, given in the column MEDV. To predict these prices, 13 features are used. For example,  the per capita crime rate of the town (column CRIM), areas of non-retail business in the town (INDUS), the age of people who own the house (AGE), and many others.\n",
    "\n",
    "The dataset is relatively small in size with only 506 cases and has the following columns (**you don't have to remember this**):\n",
    "\n",
    "| Column        | Represents                                      |\n",
    "| :------------- |------------------------------------------------ | \n",
    "| CRIM | per capita crime rate by town |\n",
    "| ZN | proportion of residential land zoned for lots over 25,000 sq.ft.|\n",
    "| INDUS | proportion of non-retail business acres per town.|\n",
    "| CHAS | Charles River dummy variable (1 if tract bounds river; 0 otherwise)|\n",
    "| NOX | nitric oxides concentration (parts per 10 million)|\n",
    "| RM | average number of rooms per dwelling|\n",
    "| AGE | proportion of owner-occupied units built prior to 1940|\n",
    "| DIS | weighted distances to five Boston employment centres|\n",
    "| RAD | index of accessibility to radial highways|\n",
    "| TAX | full-value property-tax rate per 10,000 dollar|\n",
    "| PTRATIO | pupil-teacher ratio by town|\n",
    "| B | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town|\n",
    "| LSTAT | percentage lower status of the population|\n",
    "| MEDV | Median value of owner-occupied homes (1 = 1000 dollar)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data\n",
    "We're loading this dataset from a CSV below. \n",
    "\n",
    "The first thing a data analyst does when opening a new dataset is print out a few rows to get a first impression of the data we are dealing with. We accomplish this with `.head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv(\"../data/boston_dataset.csv\")\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.head()` outputs the **first** five rows of your DataFrame by default, but we could also pass a number as well: `boston.head(10)` would output the top ten rows, for example. \n",
    "\n",
    "To see the **last** five rows use `.tail()`. `tail()` also accepts a number, and in this case we printing the bottom two rows.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically when we load in a dataset, we like to view the first couple of rows to get an intial idea of the data we are dealing with. Here we can see the names of each column, the index, and the values in each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: print first 4 rows and last 3 rows of the boston data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting info about your data\n",
    "\n",
    "Another command that is used frequently for exploring a new data set is `.info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.info()` provides the essential details about your dataset, such as the number of rows and columns, the number of non-null values, what type of data is in each column, and how much memory your DataFrame is using. Note that in this notebook we only encounter numbers (int64 = whole numbers, float64 = numbers with decimals), but in reality there are more types of data (strings, booleans, ...)\n",
    "\n",
    "### Understanding your variables\n",
    "\n",
    "To get an idea of the data you are working with, `describe()` can be used on an entire DataFrame to get a summary of the distribution of continuous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Selecting, slicing, extracting data in pandas\n",
    "\n",
    "Up until now we've focused on some basic summaries of our data. Below we explain the methods of *selecting, slicing, and extracting* that you'll need to use constantly when working with data in Python. Let's look at working with columns first.\n",
    "\n",
    "### Selecting a column\n",
    "Using square brackets is the general way we select columns in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a column by its column name\n",
    "medv_col = boston['MEDV']\n",
    "\n",
    "#use type() to view the type of this data\n",
    "type(medv_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the selected column `medv_col` is not a DataFrame like `boston` is. Instead, `medv_col` is what we call a *Series*. It's important to note that, although many methods are the same, DataFrames and Series have different attributes, so you'll need be sure to know which type you are working with or else you will receive attribute errors. \n",
    "\n",
    "To extract a column as a *DataFrame* instead, you use square brackets to pass a list of column names. In our case that's just a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medv_col = boston[['MEDV']]\n",
    "\n",
    "type(medv_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this approach can be expanded to selecting any number of columns by adding additional column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = boston[['RM', 'MEDV']]\n",
    "\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a row\n",
    "Now we can select columns. But what if you want to get a specific row?\n",
    "\n",
    "For rows, we can use the `.iloc` method to **loc**ate by numerical **i**ndex. So to get the row at index 222 we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = boston.iloc[222]\n",
    "\n",
    "print(row)\n",
    "print(type(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the row number when we select row with index 222?\n",
    "Hint: indexing starts at 0 in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Above we selectec row index 222 using ```.iloc```. What it returns is a Series. How can we edit the cell above to get a DataFrame instead?\n",
    "\n",
    "**Bonus question:** and how can we use ```.iloc``` to select both row 222 and 223?\n",
    "\n",
    "Replace the ```___``` in the following cell to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = boston.iloc[___]\n",
    "\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a trick called *slicing* to select multiple rows. Slicing is done using square brackets like `boston[1:4]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.iloc[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional selections\n",
    "We’ve gone over how to select columns and rows, but what if we want to make a conditional selection? \n",
    "\n",
    "For example, what if we want to filter our DataFrame to show only items where the nitric oxides concentration (NOX column) exceeds 0.4 parts per million?\n",
    "\n",
    "To do that, we take a column from the DataFrame and apply a Boolean condition to it. Here's an example of a Boolean condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (boston['NOX'] > 0.4)\n",
    "\n",
    "condition.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boolean condition above returns a Series of True and False values: True for concentrations above 0.4 and False for concentrations below. \n",
    "\n",
    "We want to filter out concentrations below 0.4, in other words, we don’t want rows that are False. To return the rows where that condition is True we have to pass this operation into the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston[boston['NOX'] > 0.4].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine multiple of these boolean conditions to form more complex logic. To do this, you can use the logical operators `|` for \"or\" and `&` for \"and\". \n",
    "\n",
    "For example, if we want to know all rows that have 'NOX' smaller than 0.4 OR 'TAX' smaller than 300 we can use the following boolean condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston[(boston['NOX'] > 0.4) | (boston['TAX'] < 300)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can easily filter our data to get only the values we are interested in using Boolean conditions. Also, notice how similar filtering is to what we saw in the NumPy notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Print all rows where NOX is greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Print all rows where NOX is greater than 0.5 AND TAX is smaller or equal to 311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sorting a DataFrame\n",
    "\n",
    "To sort pandas DataFrame, you may use the ```.sort_values``` syntax in Python.\n",
    "\n",
    "Below, we'll show you 4 examples to demonstrate how to sort the data in ascending and descending order based on a specific column. Then, we examine how you sort based on multiple columns.\n",
    "\n",
    "Let's say that we want to sort our boston data set in an ascending order (low to high) by median value (column MEDV). We can do that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.sort_values(by=['MEDV']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, right? Note that unless specified otherwise, pandas will sort values in an ascending order by default.\n",
    "\n",
    "Now, sorting in descending order is actually very similar. If we want to know the unsafest towns in the data set, we can sort the data in descending order by crime rate (column CRIM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.sort_values(by=['CRIM'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting on multiple columns\n",
    "Sorting on one column usually does the trick, but what if we want to sort our data according to the following description:\n",
    "\n",
    "  - Sort rows by median value of houses (column MEDV) and sort rows with equal value by the number of old houses in the town (column AGE), all in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.sort_values(by=['MEDV', 'AGE'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:  Sort the rows by crime rate (CRIM), then by tax (TAX) and finally by nitrox concentration (NOX). All in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final challenges (optional)\n",
    "\n",
    "### Challenge 1: Find the most expensive house (MEDV) that tracts the Charles River (CHAS == 1).\n",
    "tip: use filtering with conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Find the average nitric oxides concentration (NOX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3:  \n",
    "Sort the rows by:\n",
    "\n",
    "  - highest median value (MEDV) (descending order)\n",
    "  - then by lowest tax (TAX) (ascending order)\n",
    "  - and finally by highest average number of rooms per house (RM) (descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up\n",
    "\n",
    "Exploring, cleaning, transforming, and visualization data with pandas in Python is an essential skill in data science. Just cleaning and wrangling the data is more than 80% of the job of a Data Scientist. After a few projects and some practice, you should be very comfortable with most of the basics. \n",
    "\n",
    "You can now continue with level 3, where we work on more practical applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
